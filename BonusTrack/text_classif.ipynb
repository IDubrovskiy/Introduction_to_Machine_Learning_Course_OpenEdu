{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAyzWDzFdnLa"
      },
      "source": [
        "## Text classification: Spam or Ham"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh91cKPmdnLb"
      },
      "source": [
        "In this example based on the classical dataset Spambase Dataset (https://archive.ics.uci.edu/ml/datasets/spambase) we will try to make our own spam filter using scikit-learn library. The dataset contains text corpora of  5.574 text messages with labels \"spam\" or \"ham\". "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Bf-AmgdnLb"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWvXDXKrdnLb"
      },
      "source": [
        "Data are attached to the task description for your convinience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UnJwvQzbdnLb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('3_data.csv', encoding='latin-1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRqzedIIdnLc"
      },
      "source": [
        "We delete all other columns except for two of interest: text messages and labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "xO59-HEadnLc",
        "outputId": "2d14015f-89eb-48a9-b726-65078de910a6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                               text\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df[['v1', 'v2']]\n",
        "df = df.rename(columns = {'v1': 'label', 'v2': 'text'})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU2xwmvIdnLd"
      },
      "source": [
        "Delete duplicates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iveCG_bXdnLd"
      },
      "outputs": [],
      "source": [
        "df = df.drop_duplicates('text')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuPip3mzdnLd"
      },
      "source": [
        "Change labels to binary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JsKdfy6-dnLd"
      },
      "outputs": [],
      "source": [
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vQJK8LNdnLe"
      },
      "source": [
        "### Text pre-processing (Task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8tefVdTdnLe"
      },
      "source": [
        "We need to complete the function for text pre-processing, to pre-process the text the following way:\n",
        "* convert text to lowercase;\n",
        "* remove stop-words;\n",
        "* remove punctuation marks;\n",
        "* normalizes the text using Snowball stemmer.\n",
        "\n",
        "We recommend to use the NLTK library, in order not to compile a list of stop-words and not to implement the stemming algorithm yourself. Click the link to find the examples of stemmers application (https://www.nltk.org/howto/stem.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to convert \n",
        "def listToString(s):\n",
        "   \n",
        "    # initialize an empty string\n",
        "    str1 = \" \"\n",
        "   \n",
        "    # return string \n",
        "    return (str1.join(s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhU1BvAHdnLe",
        "outputId": "7b9472ba-77f2-4edb-c699-0cabf0155d8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk import stem\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stemmer = stem.SnowballStemmer('english')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    filtered_sentence = [stemmer.stem(w.lower()) for w in text.split(sep=\" \") if not w.lower() in stopwords]\n",
        "    filtered_sentence = listToString(filtered_sentence)\n",
        "    \n",
        "    # your code here\n",
        "    return filtered_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM-Lrt6ddnLe"
      },
      "source": [
        "Check that the function works correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RSuPqUb2dnLe"
      },
      "outputs": [],
      "source": [
        "assert preprocess(\"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\") == \"im gonna home soon dont want talk stuff anymor tonight k ive cri enough today\"\n",
        "assert preprocess(\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\") == \"go jurong point crazi avail bugi n great world la e buffet cine got amor wat\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84H8mOmpdnLe"
      },
      "source": [
        "Apply to the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       go jurong point crazi avail bugi n great world...\n",
              "1                                   ok lar joke wif u oni\n",
              "2       free entri 2 wkli comp win fa cup final tkts 2...\n",
              "3                     u dun say earli hor u c alreadi say\n",
              "4               nah dont think goe usf live around though\n",
              "                              ...                        \n",
              "5567    2nd time tri 2 contact u u å750 pound prize 2 ...\n",
              "5568                             ì_ b go esplanad fr home\n",
              "5569                             piti  mood soani suggest\n",
              "5570    guy bitch act like id interest buy someth els ...\n",
              "5571                                       rofl true name\n",
              "Name: text, Length: 5169, dtype: object"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text'] = df['text'].apply(preprocess)\n",
        "df['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRyqtqUtdnLe"
      },
      "source": [
        "### Split the data to the training and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nt7Z5NCMdnLe"
      },
      "outputs": [],
      "source": [
        "y = df['label'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r4PJBctdnLe"
      },
      "source": [
        "Now we need to split the data to test (test) and training (train) sets. Scikit-learn library contains ready to use tools to do it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1c9ARWIdnLe",
        "outputId": "02c80038-517d-42d9-d3f7-cfcd76d41c4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5468    urgent last weekend draw show å1000 cash spani...\n",
              "2070    sexi singl wait text age follow gender wither ...\n",
              "5332                      think steyn sure get one wicket\n",
              "204                                    u call alter 11 ok\n",
              "3394                                                  buy\n",
              "                              ...                        \n",
              "4140    beauti truth  express face could seen everyon ...\n",
              "2358           ill talk other probabl come earli tomorrow\n",
              "4848    either way work  ltgt  year old hope doesnt bo...\n",
              "3144                                ill get tomorrow send\n",
              "5408                                                  pub\n",
              "Name: text, Length: 1293, dtype: object"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.25, random_state=63)\n",
        "X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOV7Ub4ldnLe"
      },
      "source": [
        "### Classifier training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enAzNefqdnLe"
      },
      "source": [
        "We came to the classifier training now.\n",
        "\n",
        "First we extract features from the texts. It is strongly recommened to try several methods in order to check how each method influences the result (more information on defferent text representation methods you can find on the link https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction).\n",
        "\n",
        "Then we train the classifier. We use SVM, but you can try different algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QtfcmJ7NdnLe"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "# exctract features from the texts\n",
        "vectorizer = TfidfVectorizer(decode_error='ignore')\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PIQQo8j3dnLe"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#train SVM model\n",
        "\n",
        "model = LinearSVC(random_state = 63, C = 1.5)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn9kvQaZdnLe"
      },
      "source": [
        "Selfcheck. If the function ```preprocess``` is complimented correctly, then you should get the following model evaluation results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.980     0.996     0.988      1133\n",
            "           1      0.972     0.856     0.910       160\n",
            "\n",
            "    accuracy                          0.979      1293\n",
            "   macro avg      0.976     0.926     0.949      1293\n",
            "weighted avg      0.979     0.979     0.979      1293\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, predictions, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nffLu6UdnLf"
      },
      "source": [
        "Let's predict results for the specified text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "prWswDzudnLf"
      },
      "outputs": [],
      "source": [
        "txt = \"Machine factory for sale. Low price. 2 hectare property, 150,000 square feet production floor, 500 machine tools installed.\"\n",
        "txt = preprocess(txt)\n",
        "txt = vectorizer.transform([txt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brfpnzR7dnLf",
        "outputId": "0c9a82d0-7787-4f95-abed-49e742700b42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0], dtype=int64)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.predict(txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gQ4z-NbIkQO"
      },
      "source": [
        "The message is classified as spam."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "3_Students_1_en.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
